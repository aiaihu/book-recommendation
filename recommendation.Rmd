
---
Title: "Book Recommendation"
Course ID: "CSDA1040"
Professor: "Mr. Hashmat"
Author: "Group Project: Aimin Amy Hu, Jacob Geeves, Eugene Park"
Date: '2019-May-26'
output:
  html_document: 
    self_contained: no
  pdf_document: default
---



## Getting started
To work with R Markdown, if necessary:

* Install [R](http://www.r-project.org/)
* Install the lastest version of [RStudio](http://rstudio.org/download/) (at time of posting, this is 0.96)
* Install the latest version of the `knitr` package: `install.packages("knitr")`

To run the basic working example that produced this blog post:

* Open R Studio, and go to File - New - R Markdown
* If necessary install `ggplot2` and `lattice` packages: `install.packages("ggplot2"); install.packages("lattice") `
* Paste in the contents of this gist (which contains the R Markdown file used to produce this post) and save the file with an `.rmd` extension
* Please also install.package("pROC")
* Click Knit HTML


## Abstract

The objective for this project is to builde a book recommender system which could suggest books to users based on similarities between books.In other words, it would recommend books that are similar to ones that a user already likes. It would look into similar books from the same genre(perhaps fantasy, science fiction, romance, thriller, mystery etc.). It can even make recommendation based on any variety of common elements such as from the same author.

## Introduction
With techlogy rapidly update every day, online shopping, social media, YouTube and Netflix and many other websites are becoming  hoter and hoter every day. When you shopping on Amazon, you properly noted there were some products recommendation below the products which you have just bought.Have you ever bought anything from their recommended products? Did the recommended product ratings affect your buying decision? How did the recommend this product to you? Instead of discuss recommendation for shopping items online, we will discuss  recommendation system for books
in this report.

## Business Objective
The objective for this project is to builde a book recommender system which could suggest books to users based on similarities between books

### Research Questions:

* Q1: Does book's rating affect book's popularity?
* Q2: How to predict user's rating or preferences for a given book?
* Q3: How variables provide a predictive estimate of the likely ?


## Part 1: Data Understanding

### Data Source

The datasets used in this project is hosted by http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/ and below is the link to the dataset.

https://github.com/zygmuntz/goodbooks-10k/releases

Downloaded dataset and saved in local computer.

We assume the dataset come from a site similar to goodreads.com but with more permissive terms of use.
Use below R code to read CSV files from local computer

```{r results='hide', message=FALSE, warning=FALSE}
#start by loading some libraries
library(recommenderlab)
library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(DT)
library(knitr)
library(grid)
library(gridExtra)
library(corrplot)
library(methods)
library(Matrix)
library(reshape2)

```
```{r}
#set up working directory - this will set the working directory to the same folder as your R studio RMD file - ensure that the CSVs outlined below are also in this folder
set_wd <- function() {
library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path 
setwd(dirname(current_path ))
print( getwd() )
}

# Read 5 CSV files along with header
books=read.csv("books.csv",header = TRUE)
book_tags=read.csv("book_tags.csv", header = TRUE)
ratings= read.csv("ratings.csv", header = TRUE)
tags = read.csv("tags.csv", header = TRUE)
to_read = read.csv("to_read.csv", header =TRUE)

```
```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

### Data Summary

This dataset contains 5 CSV files:book_tags.csv, books.csv, ratings.csv, tags.csv, to_read.csv. We used R read.csv command to read all csv files into data frame.
```{r, eval=FALSE,echo=FALSE}

#check data frame details
str(book_tags)
str(books)
str(ratings)
str(tags)
str(to_read)

```


######to_read:
Contains 912705 observations(rows) and 2 variables(columns). This data set contains user_id and book_id.


#### ratings.csv
Contains 5976479 observations(rows) and 3 variables(columns). This data set includes all users's ratings of the 10,000 books.

```{r, result='asis', echo=FALSE}
datatable(head(ratings, 10), class = "nowrap hover row-border", options = list(dom = 't',scrollX = FALSE, autoWidth = TRUE))
```

<br>
```{r, echo=FALSE}
glimpse(ratings)
```

#### books.csv
Contains 10000 observations(rows) and 23 variables(columns). This data set contains more information on the books such as author, original_publcation_year, rating,book_id etc.

```{r, result='asis', echo=FALSE}
datatable(head(books,5),  class = "nowrap hover row-border", options = list(dom = 't',scrollX = TRUE, autoWidth=TRUE, columnDefs = list(list(width = '100px', targets = c(6)),list(width = '100px', targets = c(5,6)))))
```
<br>
```{r, echo=FALSE}
glimpse(books)
```

#### book_tags.csv
Contains 999912 observations(rows) and 3 variables(columns). This data set has all tag_ids users have assigned to that books and corresponding tag_counts.

```{r, result='asis', echo=FALSE}
datatable(head(book_tags, 10), class = "nowrap hover row-border", options = list(dom = 't',scrollX = FALSE, autoWidth = TRUE))
```

<br>
```{r, echo=FALSE}
glimpse(book_tags)
```

#### tags.csv
Contains 34252 observations(rows) and 2 variables(columns). This data set includes the tag_names corresponding to the tag_ids

```{r, result='asis', echo=FALSE}
datatable(sample_n(tags, 5), class = "nowrap hover row-border", options = list(dom = 't',scrollX = FALSE, autoWidth = TRUE))
```

<br>
```{r, echo=FALSE}
glimpse(tags)
```

## Part 2: Data Prepartion

To prepare quality dataset for the machine learning, we will need to check our dataset for any duplicated elements, missing values.

### Step 1: Duplicated elements
We will check if there are duplicated elements in ratings dataset and books dataset. Some users may rating same book for twice. If this is a case, we will then remove the duplicated rows. Also, we will make sure there are not duplicated books in the books dataset. To do this, we will use dplyr. We found there are no duplicated rows in books and book_tags datasets.
```{r}
#loading library for identifying duplicated rows.
library(dplyr)
```
```{r}
#checking if there are duplicated rows

ratings <- ratings %>%
              distinct(user_id, book_id, .keep_all = TRUE)
```
```{r}
books <- books %>%
                distinct(book_id, .keep_all = TRUE)
```

### Step 2: Missing Values

Using code: colSums(is.na()) to get below display. This tells which variable has how many missing values in this data set.


```{r}

#checking missing value for the dataset
colSums(is.na(book_tags))

```
```{r}
colSums(is.na(books))

```
```{r}
colSums(is.na(ratings))

```
```{r}
colSums(is.na(tags))

```

```{r}
colSums(is.na(to_read))


```
In summary of above results, the missing values are summarized as below list:

Data set  |Variable name             | Missing/NA Values           
----------|--------------------------|-------------------
  books   | isbn13                   | 585                       
  books   | original_publication_year| 21  
          |                          |
There are only missing vaules in books_df. The missing values are isbn(International Standard Book Number) and original_publication_year. These two variables will not be variables used in a recommender system. Therefore, we will not deal these missing values. We can be confident is saying that this dataset contains good quality data.

### Step 3:Clean data

Before we use the dataset, we will do some data clean to remove unuseful values. In this way, it will make the dataset smaller and make computation faster.

*	Remove variables: books dataset
* This data set contains 585 missing values under variable 'isbn13'. We have variable 'isbn', we assume these two variables are the same, hence, we    will remove variable 'isbn13' from the data frame. 

* This data set contains variable 'best_book_id'. This id number is identical to variable 'goodreads_book_id'. Becasue variable 'goodreads_book_id' is cross referenced in the 'book_tags_df' dataset it will remain while the 'best_book_id' variable is removed.

* This data set contains variable 'work_id'. This variable will be removed as book_id and goodreads_book_id already serve to identify a unique book.


```{r}
books_df <- select(books, -isbn13, -best_book_id)
```
After removed two variables from this dataset, we are haveing 21 variables.

##Part 3: Data Exploration

### Data Exploration:ratings dataset
As we know that the ratings dataset contains user_id, book_id and rating. We use ggplot() and geom()to get visulaztion between these variables.

#### Distribution of ratings
We check distribution of ratings and found:
The most of ratings are between 3-5 and less than 500,000 ratings are in 1 and 2.

```{r}
library(ggplot2)
ratings %>% 
  ggplot(aes(x = rating, fill = factor(rating))) +
  geom_bar(color = "grey10") + scale_fill_brewer(palette = "Dark2") + guides(fill = FALSE)

```


#### Number of ratings per book
We want to check the distribution of the number of ratings for each book. Most of books has total of ratings below 1000.

```{r}
#group by book_id,then do the plot
ratings %>% 
  group_by(book_id) %>% 
  summarize(number_of_ratings_per_book = n()) %>%
  ggplot(aes(number_of_ratings_per_book)) + 
  geom_bar(fill = "green", color = "grey20",width = 1) +
  coord_cartesian(c(100, 3000))
  
```

####Number of ratings per user
People are pretty active for the ratings. Most of people have number of rating between 75 - 150 times.
```{r}
ratings %>% 
  group_by(user_id) %>% 
  summarize(number_of_ratings_per_user = n()) %>% 
  ggplot(aes(number_of_ratings_per_user)) + 
  geom_bar(fill = "cadetblue3", color = "grey20") + coord_cartesian(c(3, 200))
```

####Distribution of mean book ratings
What we have seen from below plot is the most of books have mean between 3.5- 5, this tells us that most of books get postive ratings.
```{r}
#group by book_id and get mean of rating for each book_id. We get a new dataset with column mean_perbook

rating_perbook<-ratings %>%
        group_by(book_id) %>%
        mutate(mean_perbook = mean(rating, na.rm=T))
        
```

```{r}
ggplot(rating_perbook, aes(mean_perbook))+geom_histogram(fill = "orange", color = "grey20", bins = 30) + coord_cartesian(c(1,5))
```

####Distribution of mean user ratings
From below plot, we saw that more people tend to give ratings higher than 3.5. Some of them have tendence to give highest rating 5, and another concentration area is around rating 4.

```{r}
ratings %>% 
  group_by(user_id) %>% 
  summarize(mean_per_user_rating = mean(rating)) %>% 
  ggplot(aes(mean_per_user_rating)) +
  geom_histogram(fill = "red", color = "grey20", bins =30)
```

####Does rater relate to rating?
From below plot, we see a possiblity of rating differently between frequent users and less frequent users.

```{r}
ratings %>%
  group_by(user_id) %>%
  summarize(mean_rating =mean(rating), number_of_rated_books =n())%>%
  ggplot(aes(number_of_rated_books, mean_rating))+stat_bin_hex(bins =50) +scale_fill_distiller(palette ="Spectral")+
  stat_smooth(method = "lm", color = "orchid", size = 2)+
  scale_color_gradient()+
  theme_bw()

```


### Data Exploration:books dataset

The books dataset contains 21 variables after our data cleaning. We will explore some variables in this dataset to see how they are related to the ratings.

```{r}
# list of varialbe names in the books dataset
names(books)
```
We see there are variables called language_code and authors. First, we will check how laugange related to the books. We uderstood that the dataset is from an English speaking website and assume that the most of books are English.

####Languages

From below plots, we see there are less than 1000 books are non-English.

```{r}
#import some libraries
library(grid)
library(ggplot2)
library(lattice)

```
```{r}
#language: English(this will include en-US, en-GB,eng,en-CA)
p_english <- books %>% 
  mutate(language = factor(language_code)) %>% 
  group_by(language) %>% 
  summarize(number_of_books = n()) %>% 
  arrange(-number_of_books) %>% 
  ggplot(aes(reorder(language, number_of_books), number_of_books, fill = reorder(language, number_of_books))) +
  geom_bar(stat = "identity", color = "grey20", size = 0.5) + coord_flip() +
  labs(x = "language", title = "English") + guides(fill = FALSE)

P_non_english <- books %>% 
  mutate(language = factor(language_code)) %>% 
  filter(!language %in% c("en-US", "en-GB", "eng", "en-CA", "")) %>% 
  group_by(language) %>% 
  summarize(number_of_books = n()) %>% 
  arrange(-number_of_books) %>% 
  ggplot(aes(reorder(language, number_of_books), number_of_books, fill = reorder(language, number_of_books))) +
  geom_bar(stat = "identity", color = "grey20", size = 0.5) + coord_flip() +
  labs(x = "", title = "Non-English") + guides(fill = FALSE)

grid.arrange(p_english,P_non_english, ncol=2)

```


####Authors
We see from below plot, there are a few authors with lower rating below 3,which may indicate that rating is high or low not related to authors.
```{r}
books %>% 
  group_by(authors, average_rating) %>% 
  summarize(mean_per_authors_rating = mean(average_rating)) %>% 
  ggplot(aes(mean_per_authors_rating)) +
  geom_histogram(fill = "red", color = "grey20", bins =30)

```


However,we saw number of authors is a matter of the rating from below plot.The more authors a book has the higher average rating is. 

```{r}
books_author <- books %>% 
  group_by(book_id) %>% 
  mutate(number_of_authors = length(str_split(authors, ",")[[1]]))

books_author %>% filter(number_of_authors <= 10) %>% 
  ggplot(aes(number_of_authors, average_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller(palette = "Spectral") +
  stat_smooth(method = "lm", size = 2, color = "orchid", se = FALSE)+
  scale_color_gradient()+
  theme_bw()

```


#### Does average_rating relate to ratings_count?
From below plot, we see there are only in a small extent, average_rating is higher and number of ratings_count is higher.

```{r}
ggplot(books,aes(x=ratings_count, y=average_rating))+
  geom_point(aes(color = books_count)) +
  stat_smooth(method = "lm", color = "orchid", size = 2)+
  scale_color_gradient()+
  theme_bw()
  
       
```


#### Top 10 books in average_rating  
We rank the top 10 rated books according to their average_rating. The Complete Calvin and Hobbes has the highest average_rating at 4.82, it's so close to 5. With 28900 ratings_count, we see almose every one gives this book high rating.  
```{r}
#import library
library(DT)
library(dplyr)
```
```{r}
#use arrange()to sort variable avaerage_rating, then get the top 10 books with title, ratings_count, average_rating and authors
  books %>% 
  arrange(-average_rating) %>% 
  top_n(10,wt = average_rating) %>% 
  select(title, ratings_count, average_rating, authors) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))

```


#### Top 10 books in ratings_count  
We rank the top 10 books according to their ratings_count. We saw some books on the list but their average_rating is less than mean of average_rating. This tells us a book is popular to be rated, but it doesn't mean that this book has higher average_rating.


```{r}
#use arrange()to sort variable ratings_count, then get the top 10 books with title, ratings_count, average_rating and authors
  books %>% 
  arrange(-ratings_count) %>% 
  top_n(10,wt = ratings_count) %>% 
  select(title, ratings_count, average_rating, authors) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```
```{r}
# mean of average_rating
mean(books$average_rating)
```

### Correlation Matrix

A correlation matrix is a table showing correlation coefficients between variables. The books contains many variables. Some of variables may not have correlation or weak correlation with other variables. Use corrplot() function to see Visualization of Correlation Matrix.

We see some correlations from below correlation martrix plot.

* Variable ratings_count has positive 1 correlation with variable work_ratings_count. Does this mean that they are same vaules but just called different variable name? We are not sure about this as the discrepation about the dataset did not specify it.

* Variable work_text_reviews_count has positive 0.8 correlation with ratings_count and work_ratings_count.

* Variable books_count has positive correlation with variable work_text_reviews_count, ratings_count and work_ratings_count; this variable also has strong negative correlation with variable original_publication_year, this does make sense as the original_publication_year increases, the books_count decreases. Variable books_count also has negative correlation with average_rating, but not strong, is just about -0.1.


```{r}
#correlation matrix, select variables with numeric vaules
books_subset <- books %>% 
  select(one_of(c("books_count","original_publication_year","ratings_count", "work_ratings_count", "work_text_reviews_count", "average_rating"))) %>% 
  as.matrix()
corrplot(cor(books_subset,use = 'pairwise.complete.obs'), type = "upper", order = "hclust")
        
```


###Conclusion
We have explored ratings dataset and books dataset and identified some variables that affect on book ratings, this will help us to recommend a book. However,these dataset does not include many other aspects such as user's age group, income, education, culture backgroud etc. We are unclear how these aspects influnce on the book ratings.


## Part 4: Build Model -Collaborative Filtering
A recommendation system is a type of information filter, which can learn usersâ€™ interests and hobbies according to their profile or historical behaviors, and then predict their ratings or preferences for a given item. It changes the way businesses communicate with users and strengthens the interactivity between them.

There are two types of filtering systems that we have used for the prediction.

**User-based collaborative filtering systems**: A user-based recommendation engine recommends products such as movies or online shopping items based on what other users with similar profiles have watched/liked or bought in the past. 


**Item-based collaborative filtering systems**: An item-based recommender would make recommendations based on similarities bewtween products. It would recommend the products that are similar to ones that you already like. For example, if a user readed  "Little Women" and liked it so this user given it rating 5 stars. A item-based collaborative filtering system would then look into similar books from the same genre(perhaps romance,true love, womenhood or based on similar storyline) and then recommend to the user similar books based on the preference the user indicated when he/she gave "Little Women" rating. An item-based collaborative filtering system can also make recommendations based on any other variety of common elements such as from the same author.


When we do the recommendation for a book, we are not just only lookig at the quantity of read/viewed, we have considered the rating that each of the users gave the book. This will help us to identify books that similiar users
have liked, then filter our book recommendations accordingly. With saying this, the recommendation will only include the books with rated high by other similar users.

### Split Dataset
We need to structure our data like this: each row corresponds to a user and each column corresponds to a book. To restructure our dataset, we first to split the dataset into train and test dataset.
train =60% of dataset
test =40% of dataset

```{r}
library(caTools)
```

```{r}
#To set a seed number
set.seed(1)

#Split dataset. 0.6 indicates 60% training and 40% test data.
spl<-sample.split(ratings,SplitRatio = 0.6)

```
```{r}
#spl
train=subset(ratings,spl=="FALSE")
test=subset(ratings,spl=="TRUE")

```
To restructure our train dataset by create a ratings matrix with rows as users and columns as books
```{r}

#Create ratings matrix with rows as users and columns as books.
install.packages("reshape2", dependencies=TRUE)
install.packages("stringi", dependencies=TRUE)
library(stringi)
library(reshape2)
```

```{r}
#creat a dimension_names list based on the train dataset
dimension_names = list(user_id =sort(unique(train$user_id)), book_id=sort(unique(train$book_id)))
```

```{r}
#import some libraries
library(dplyr)
library(tidyverse)

```

```{r}
#Spread a key-value pair across multiple columns.
ratingmat <- spread(select(train, book_id, user_id, rating), book_id, rating) %>%
  select(-user_id)
    
```
```{r}
#Create train matrix with rows as user_id and columns as book_id

ratingmat = as.matrix(ratingmat)
```

```{r}
# check the class ratingmat
class(ratingmat)

```

```{r}
#set the dimnames of matrix
dimnames(ratingmat)<-dimension_names
ratingmat[1:5, 1:5]
```


```{r}
#Convert ratings matrix to real rating matrx which makes it dense
library(recommenderlab)
ratingmat = as(ratingmat,"realRatingMatrix")

```
By running above code, we reduced the size of ratingmat file to 49.5mb


### Normalize the matrix
```{r}
ratingmat_n =normalize(ratingmat)


```
Above codes are as of May 21 at 5:50.
changes made a lot after yesterday push which include re-wirte words and part 4