
---
Title: "Book Recommendation"
Course ID: "CSDA1040"
Professor: "Mr. Hashmat"
Author: "Group Project: Aimin Amy Hu, Jacob Geeves"
Date: '2019-May-26'
output:
  html_document: 
    self_contained: no
  pdf_document: default
---



## Getting started
To work with R Markdown, if necessary:

* Install [R](http://www.r-project.org/)
* Install the lastest version of [RStudio](http://rstudio.org/download/) (at time of posting, this is 0.96)
* Install the latest version of the `knitr` package: `install.packages("knitr")`

To run the basic working example that produced this blog post:

* Open R Studio, and go to File - New - R Markdown
* If necessary install `ggplot2` and `lattice` packages: `install.packages("ggplot2"); install.packages("lattice") `
* Paste in the contents of this gist (which contains the R Markdown file used to produce this post) and save the file with an `.rmd` extension
* Please also install.package("pROC")
* Click Knit HTML


## Abstract

The objective for this project is to builde a book recommender system which could suggest books to users based on similarities between books.In other words, it would recommend books that are similar to ones that a user already likes. It would look into similar books from the same genre(perhaps fantasy, science fiction, romance, thriller, mystery etc.). It can even make recommendation based on any variety of common elements such as from the same author.

## Introduction



## Business Objective
The objective for this project is to builde a book recommender system which could suggest books to users based on similarities between books



## Part 1: Data Understanding

### Data Source

The datasets used in this project is hosted by http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/ and below is the link to the dataset.

https://github.com/zygmuntz/goodbooks-10k/releases

Downloaded dataset and saved in local computer.

We assume the dataset come from a site similar to goodreads.com but with more permissive terms of use.
Use below R code to read CSV files from local computer

```{r}
#start by loading some libraries
library(recommenderlab)
library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(DT)
library(knitr)
library(grid)
library(gridExtra)
library(corrplot)
library(methods)
library(Matrix)
library(reshape2)

```
```{r}
#set up working directory
setwd("~/York U School/CSDA1040/Project_1/Datasets")

# Read 5 CSV files along with header
books=read.csv("books.csv",header = TRUE)
book_tags=read.csv("book_tags.csv", header = TRUE)
ratings= read.csv("ratings.csv", header = TRUE)
tags = read.csv("tags.csv", header = TRUE)
to_read = read.csv("to_read.csv", header =TRUE)

```

## Data Summary

This dataset contains 5 CSV files:book_tags.csv, books.csv, ratings.csv, tags.csv, to_read.csv. We used R read.csv command to read all csv files into data frame.
```{r, eval=FALSE,echo=FALSE}

#check data frame details
str(book_tags)
str(books)
str(ratings)
str(tags)
str(to_read)

```

######books:
Contains 10000 observations(rows) and 23 variables(columns). This data set contains more information on the books such as author, original_publcation_year, rating,book_id etc.

######book_tags:
Contains 999912 observations(rows) and 3 variables(columns). This data set has all tag_ids users have assigned to that books and corresponding tag_counts.

######rating:
Contains 5976479 observations(rows) and 3 variables(columns). This data set includes all users's ratings of the books.

######tags:
Contains 34252 observations(rows) and 2 variables(columns). This data set includes the tag_names corresponding to the tag_ids

######to_read:
Contains 912705 observations(rows) and 2 variables(columns). This data set contains user_id and book_id.

###Use head()command to show first 6 rows of each data frame

```{r, echo=FALSE}

#show first 6 rows of book_tags_df
head(book_tags)

```
```{r, echo=FALSE}
#show first 6 rows of books_df
head(books)

```
```{r,echo=FALSE}
#show first 6 rows of ratings_df
head(ratings)

```
```{r, echo=FALSE}
#show first 6 rows of tags_df
head(tags)

```
```{r, echo=FALSE}
#show first 6 rows of to_read_df
head(to_read)
```


## Data Exploration

### Missing Values

Using code: colSums(is.na()) to get below display. This tells which variable has how many missing values in this data frame.


```{r}

#checking missing value for the dataset
colSums(is.na(book_tags))

```
```{r}
colSums(is.na(books))

```
```{r}
colSums(is.na(ratings))

```
```{r}
colSums(is.na(tags))

```

```{r}
colSums(is.na(to_read))


```
In summary of above table, the missing values are summarized as below list:


Variable name             | Missing/NA Values           
------------------------- | ------------------
isbn13                    | 585                       
original_publication_year | 21  

There are only missing vaules in books_df. The missing valuse are isbn(International Standard Book Number) and original_publication_year. These two variables will not be variables for recommender system. Therefore, we will not deal thse missing values. We are confident to say this dataset has good quality of data.

##Part 2: Data Preparation

### Clean data

Before we use the books dataset, we will do some data clean to remove unuseful values. In this way, it will make the dataset smaller and make computation faster.

  *	This data frame contains 585 missing value in variable "isbn13". We have variable "isbn", we assume these two variables are same, hence, we will remove variable"isbn13"from the data frame. 
  *On this data set, it als contains a variable called: best_book_id. This id number is same as variable: goodreads_book_id. We will keep goodreads_book_id as this variable is also corresponding in book_tags_df dataset, but will remove best_book_id variable.
  * work_id on this data set will be removed as we have book_id and goodreads_book_id to indicate a unique book.

```{r}
#check if there are dupicatie elements in datasets
tags[duplicated(tags)]
```
```{r}

#Create ratings matrix with rows as users and columns as books. 
ratingmat = dcast(ratings, user_id~book_id, value.var = "rating", na.rm=FALSE)
```
```{r}
# check the class ratingmat
class(ratingmat)

```
```{r}
# remove user_id as we don't need it
ratingmat = as.matrix(ratingmat[,-1])
```

```{r}
#Convert ratings matrix to real rating matrx which makes it dense
ratingmat = as(ratingmat,"realRatingMatrix")

```
By running above code, we reduced the size of ratingmat file to 69mb


### Normalize the matrix
```{r}
ratingmat_n =normalize(ratingmat)


```

###Create Recommender Model. The parameters are UBCF and Cosine similarity. We take 10 nearest neighbours
```{r}
rec_mod = Recommender(ratingmat_n,method = "UBCF",param =list(method ="Cosine",nn=10))
```
### Obtain top 5 recommendations for 1st user entry in dataset
```{r}
Top_5_pred = predict(rec_mod, ratingmat_n[1],n=5)

```
```{r}
#convert the recommendations to a list
Top_5_list = as(Top_5_pred, "list")
Top_5_list
```
We got book recommendations for the top 5 books, but they are in book_id number format. Now, we need to look at the book names that correspond to these book_id number.We will do this by using the books dataset. It maps book_id to book tiles.


  
```{r}
# Part 2: Data Preparation

# clean the data

```
  *	Convert date format for variable BIRTH.
```{r}
claim_df$BIRTH=as.Date(chron(format(as.Date(claim_df$BIRTH,"%d%b%y"),"%m/%d/%y")))
str(claim_df$BIRTH)
```
  *	Remove dollar sign $ from data frame.
```{r}
claim_df[]<-lapply(claim_df,gsub,pattern="\\$",replacement="")

```
  *	Remove " ," from data frame.
```{r}
claim_df[]<-lapply(claim_df,gsub,pattern=",",replacement="")
```
  *	Convert characters to numeric for INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM, CLM_AMT
```{r}
claim_df$INCOME<-as.numeric(claim_df$INCOME)
claim_df$HOME_VAL<-as.numeric(claim_df$HOME_VAL)
claim_df$BLUEBOOK<-as.numeric(claim_df$BLUEBOOK)
claim_df$OLDCLAIM<-as.numeric(claim_df$OLDCLAIM)
claim_df$CLM_AMT<-as.numeric(claim_df$CLM_AMT)
```

Now lets see the cleaned up dataset structure and summary

```{r, eval=FALSE}
str(claim_df)
summary(claim_df)
```

###Analysis variables

After the data acquisition from the anonymous car insurance provider, I assume that this dataset represents recent values. The analysis of this dataset is expected results in certain correlations: how each predictor variable relates to our target variable CLAIM_FLAG?
Barplot and histogram are visualizations that are useful and direct way to tell story about each variable relates to the target variable.

```{r}
ggplot(claim_df,aes(x=KIDSDRIV,fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar()+
  labs(y="Policyholder Count",
       title="past accident rates by KIDSDRIV")

#gender and occupation relations with past accident
ggplot(claim_df,aes(x=GENDER,fill=CLAIM_FLAG))+
  theme_bw()+
  facet_wrap(~OCCUPATION)+
  geom_bar()+
  labs(y="Policyholder Count",
       title="Past Accident rate by gender and occupation")

#use barplot to see age and past accident relation
claim_df$AGE<-as.numeric(claim_df$AGE)
ggplot(claim_df,aes(x=AGE, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by Age")

#plot for YOJ 
claim_df$YOJ<-as.numeric(claim_df$YOJ)
ggplot(claim_df,aes(x=YOJ, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by Years of Job")

#plot variable HOMEKIDS with accident
ggplot(claim_df,aes(x=HOMEKIDS, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by HOMEKIDS")

#plot variable MSTATUS with accident
ggplot(claim_df,aes(x=MSTATUS, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by MSTSTUS")

#plot variable EDUCATION with accident
ggplot(claim_df,aes(x=EDUCATION, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by EDUCATION")

#plot variable TRAVTIME with accident
claim_df$TRAVTIME<-as.numeric(claim_df$TRAVTIME)
ggplot(claim_df,aes(x=TRAVTIME, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by TRAVTIME")

#plot variable CAR_USE with accident
ggplot(claim_df,aes(x=CAR_USE, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by CAR_USE")

#plot variable BLUEBOOK with accident
ggplot(claim_df,aes(x=BLUEBOOK, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_histogram(bins=30) +
  labs(y="Policyholder Count",
       x="BLUEBOOK",
       title = "Accident Rates by BLUEBOOK")

#plot variable TIF with accident
claim_df$TIF<-as.numeric(claim_df$TIF)
ggplot(claim_df,aes(x=TIF, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by TIF")

#plot variable CAR_TYPE with accident
ggplot(claim_df,aes(x=CAR_TYPE, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by Car Type")

#plot variable RED_CAR with accident
ggplot(claim_df,aes(x=RED_CAR, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       title = "Accident Rates by Red Car")

#plot variable OLDCLAIM with accident

ggplot(claim_df,aes(x=OLDCLAIM, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_histogram(bins=20) +
  labs(y="Policyholder Count",
       x="OLDCLAIM",
       title = "Accident Rates by Old Claim")

#plot variable CLM_FREQ with accident
ggplot(claim_df,aes(x=CLM_FREQ, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar() +
  labs(y="Policyholder Count",
       x="CLM_FREQ",
       title = "Accident Rates by CLM_FREQ")

#plot variable REVOKED with accident
ggplot(claim_df,aes(x=REVOKED, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar()+
  labs(y="Policyholder Count",
       title = "Accident Rates by REVOKED")


#plot variable MVR_PTS with accident
claim_df$MVR_PTS<-as.numeric(claim_df$MVR_PTS)
ggplot(claim_df,aes(x=MVR_PTS, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar()+
  labs(y="Policyholder Count",
       title = "Accident Rates by MVR_PTS")

#plot variable CAR_AGE with accident
claim_df$CAR_AGE<-as.numeric(claim_df$CAR_AGE)
ggplot(claim_df,aes(x=CAR_AGE, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar()+
  labs(y="Policyholder Count",
       title = "Accident Rates by Car Age")

#plot variable URBANICITY with accident
ggplot(claim_df,aes(x=URBANICITY, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_bar()+
  labs(y="Policyholder Count",
       title = "Accident Rates by Urbanicity")

#plot variable INCOME with accident
ggplot(claim_df,aes(x=INCOME, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_histogram(bins=30) +
  labs(y="Policyholder Count",
       x="INCOME",
       title = "Accident Rates by INCOME")

#plot variable HOME_VAL with accident
ggplot(claim_df,aes(x=HOME_VAL, fill=CLAIM_FLAG))+
  theme_bw()+
  geom_histogram(bins=30) +
  labs(y="Policyholder Count",
       x="HOME_VAL",
       title = "Accident Rates by Home Value")
```

## Part 2: Data Preparation

### Step 1: Dealing with Categorical Variables

***Note:*** The table above shows the columns that we will convert into categorical variables for our Logistical Regression Model.


<center><img src="./img1.png"></center>
\newline

```{r}
#Converting GENDER variable to numeric: male=1, female=0
claim_df$GENDER[claim_df$GENDER=="M"] <- "1"
claim_df$GENDER[claim_df$GENDER=="F"] <- "0"

#need to convert character to numeric
claim_df$GENDER<-as.numeric(claim_df$GENDER)

#Converting PARENT1 variable to numeric: Yes=1, No=0
claim_df$PARENT1[claim_df$PARENT1=="Yes"] <- "1"
claim_df$PARENT1[claim_df$PARENT1=="No"] <- "0"

#need to convert character to numeric
claim_df$PARENT1<-as.numeric(claim_df$PARENT1)


#Converting MSTATUS variable to numeric: Yes=1, No=0
claim_df$MSTATUS[claim_df$MSTATUS=="Yes"] <- "1"
claim_df$MSTATUS[claim_df$MSTATUS=="No"] <- "0"

#need to convert character to numeric
claim_df$MSTATUS<-as.numeric(claim_df$MSTATUS)


#Converting CAR_USE variable to numeric: Commercial =1, Private =0
claim_df$CAR_USE[claim_df$CAR_USE=="Commercial"] <- "1"
claim_df$CAR_USE[claim_df$CAR_USE=="Private"] <- "0"

#need to convert character to numeric
claim_df$CAR_USE<-as.numeric(claim_df$CAR_USE)

#Converting RED_CAR variable to numeric: yes =1, no=0
claim_df$RED_CAR[claim_df$RED_CAR=="yes"] <- "1"
claim_df$RED_CAR[claim_df$RED_CAR=="no"] <- "0"

#need to convert character to numeric
claim_df$RED_CAR<-as.numeric(claim_df$RED_CAR)


#Converting REVOKED variable to numeric: Yes=1, No=0
claim_df$REVOKED[claim_df$REVOKED=="Yes"] <- "1"
claim_df$REVOKED[claim_df$REVOKED=="No"] <- "0"

#need to convert character to numeric
claim_df$REVOKED<-as.numeric(claim_df$REVOKED)

#Converting URBANICITY variable to numeric: Highly Urban/ Urban =1
#Highly Rural/ Rural=0
claim_df$URBANICITY[claim_df$URBANICITY=="Highly Urban/ Urban"] <- "1"
claim_df$URBANICITY[claim_df$URBANICITY=="Highly Rural/ Rural"] <- "0"

#need to convert character to numeric
claim_df$URBANICITY<-as.numeric(claim_df$URBANICITY)

#Converting EDUCATION variale to numeric: High School=1, Bachelors=2
#Masters=3,PhD=4
claim_df$EDUCATION[claim_df$EDUCATION=="High School"] <- "1"
claim_df$EDUCATION[claim_df$EDUCATION=="Bachelors"] <- "2"
claim_df$EDUCATION[claim_df$EDUCATION=="Masters"] <- "3"
claim_df$EDUCATION[claim_df$EDUCATION=="PhD"] <- "4"

#need to convert character to numeric
claim_df$EDUCATION<-as.numeric(claim_df$EDUCATION)


#Converting CAR_TYPE variable to numeric: Minivan or Van = 1, Sport Cars=2
#Suv=3, Panel Truck =4, Pickup=5
claim_df$CAR_TYPE[claim_df$CAR_TYPE=="Minivan"|claim_df$CAR_TYPE=="Van"] <- "1"
claim_df$CAR_TYPE[claim_df$CAR_TYPE=="Sports Car"] <- "2"
claim_df$CAR_TYPE[claim_df$CAR_TYPE=="SUV"] <- "3"
claim_df$CAR_TYPE[claim_df$CAR_TYPE=="Panel Truck"] <- "4"
claim_df$CAR_TYPE[claim_df$CAR_TYPE=="Pickup"] <- "5"

#need to convert character to numeric
claim_df$CAR_TYPE<-as.numeric(claim_df$CAR_TYPE)



#replace NA with Not Specified for OCCUPATION
claim_df$OCCUPATION<-claim_df$OCCUPATION %>% replace_na(("Not Specified"))

#Converting OCCUPATION variable to numeric:Not Specified =0, Student =1,Blue Collar or Home Maker=2
#Manager or Clerical=3, Professional =4, Doctor or Lawyer =5

claim_df$OCCUPATION[claim_df$OCCUPATION=="Not Specified"] <- "0"
claim_df$OCCUPATION[claim_df$OCCUPATION=="Student"] <- "1"
claim_df$OCCUPATION[claim_df$OCCUPATION=="Home Maker" |claim_df$OCCUPATION=="Blue Collar" ] <- "2"
claim_df$OCCUPATION[claim_df$OCCUPATION=="Manager" |claim_df$OCCUPATION=="Clerical" ] <- "3"
claim_df$OCCUPATION[claim_df$OCCUPATION=="Professional"] <- "4"
claim_df$OCCUPATION[claim_df$OCCUPATION=="Doctor" |claim_df$OCCUPATION=="Lawyer" ] <- "5"
#need to convert character to numeric
claim_df$OCCUPATION<-as.numeric(claim_df$OCCUPATION)

#need to convert character to numeric
claim_df$KIDSDRIV<-as.numeric(claim_df$KIDSDRIV)
claim_df$HOMEKIDS<-as.numeric(claim_df$HOMEKIDS)
claim_df$CLM_FREQ<-as.numeric(claim_df$CLM_FREQ)
claim_df$CLAIM_FLAG<-as.numeric(claim_df$CLAIM_FLAG)

```

### Step 2: Dealing with missing values

\newline

#### AGE

We found a total of 7 missing values for the AGE variable. Since the data set is comprised of policyholder information, the missing AGE values required imputation. Replace missing values with median AGE value without significantly introducing bias into the dataset.

```{r}
#calculate median for age
agemedian<-median(claim_df$AGE,na.rm = TRUE)

#replace NA with median for AGE
claim_df$AGE<-claim_df$AGE %>% replace_na((agemedian))

```

#### CAR_AGE

There are 639 missing values in CAR_AGE variable. As a required imputation, it is reasonable to replace it with median based on plot in part 1. We also found that there are negative values in this variable, it is reasonable to assume there were typos. It will be replaced with absolute value. For values are zero, we will replace it with 1 as it might be an error when policyholders were applying insurance, the car was just brand new.

```{r}
#Replace negative values with absolute value for CAR_AGE
claim_df$CAR_AGE[claim_df$CAR_AGE==-3]<-3

#Replace zero with 1 for CAR_AGE
claim_df$CAR_AGE[claim_df$CAR_AGE==0]<-1

#calculate median for CAR_AGE
carage_median<-median(claim_df$CAR_AGE,na.rm = TRUE)
mean(claim_df$CAR_AGE,na.rm = TRUE)

#Replace NA with median for CAR_AGE
claim_df$CAR_AGE<-claim_df$CAR_AGE %>% replace_na((carage_median))
claim_df$CAR_AGE <- as.numeric(claim_df$CAR_AGE)
```

#### OCCUPATION

There are 665 missing values in OCCUPATION variable. It is reasonable to assume not every policy holder was employed or information might not be collected when the policyholder was applying for insurance. As a required imputation, we created a new category called "Not Specified"" for these 665 values.

```{r, eval=FALSE}
#replace NA with Not Specified for OCCUPATION
claim_df$OCCUPATION<-claim_df$OCCUPATION %>% replace_na(("Not Specified"))
```

#### INCOME, HOME_VAL, YOJ

For missing values in these 3 variables, we will replace them with mean. We assume that replaced values will not bring significant bias for the dataset since the percentage of missing values is just about 5.5% and also the plot in part 1 showed us these 3 variables seems not have strong correlations with our target variable CLAIM_FLAG.

Below show the R code that deal with missing values with replacement:

```{r}

#Replace missing value by mean for INCOME variable
incomeMean = claim_df$INCOME %>% mean(na.rm=TRUE)
claim_df$INCOME <- claim_df$INCOME %>% replace_na(incomeMean)

#Replace missing value by mean for HOME_VAL variable
homeValueMean = claim_df$HOME_VAL %>% mean(na.rm=TRUE)
claim_df$HOME_VAL <- claim_df$HOME_VAL %>% replace_na(homeValueMean)

#Replace missing value by mean for YOJ variable
YOJMean = claim_df$YOJ %>% mean(na.rm=TRUE)
claim_df$YOJ <- claim_df$YOJ %>% replace_na(YOJMean)

```

### Step 3: Correlation Matrix

A correlation matrix is a table showing correlation coefficients between variables. This dataset contains many variables. Some of variables may not have correlation or weak correlation with other variables. Use corrplot() function to see Visualization of Correlation Matrix.

```{r}

#Step 3: Correlation matrix
#subset dataset without ID,BIRTH and CLM_AMT and a few others.
cl_df<-subset(claim_df,select=-c(ID,BIRTH,CLM_AMT,YOJ,TRAVTIME,MVR_PTS,TIF))

#mdf: correlation matrix
round(cor(cl_df),3)
mdf<-round(cor(cl_df),3)

#plot for correlation matrix
library(corrplot)
corrplot(mdf, type="upper", method = "circle",insig = "blank", order="hclust", diag=FALSE)
#corrplot(mdf, method = "circle",insig = "blank")
```

### Step 4: Select Data

Based on above correlation matrix plot, we decided to select variables with correlation matrix absolute greater than 0.45
Below code and generated output table shows correlation matrix absolute greater than 0.45

```{r}
#anoth way to find out strong correlation between variables
#drop duplicates and meaningless information. Only retain the upper half of the correlation matrix for ease of computation
mdf[lower.tri(mdf,diag = TRUE)]=NA
# Convert into a dataframe for manipulation
matrix_df<-as.data.frame(as.table(mdf))

library(reshape)
#drop perfect ("1") as it provides no insight into the relationship
matrix_df[matrix_df==1]<-NA
#subset correlation matrix with absolute greater than 0.45, as we are looking for strong correlationship vars
matrix_df10<-subset(matrix_df,abs(Freq)>0.45)
#sort the correlationship matrix in decending order.
# Here we can clearly see EDUCATION and CAR_AGE has the strongest relationship at 0.675
matrix_df10[order(-abs(matrix_df10$Freq)),]
```

Therefore, the selected variables will be 14 as below:  **EDUCATION, CAR_AGE, GENDER, RED_CAR, INCOME, HOME_VAL, OLDCLAIM, CLM_FREQ, PARENT1, MSTATUS, KIDSDRIV, HOMEKIDS, OCCUPATION, CAR_USE**

Response variable:  **CLAIM_FLAG**


## Part 3 - Build Models

### Split Dataset

train =60% of dataset
test =40% of dataset

### Create Models I: Logistic Regression Model

*Note: We shall use Logistic Regression Model - Using Backward Selection*

**CLAIM_FLAG** attribute as the response variable, while various subsets of the 14 potential predictor variables were used as independent variables.

Our response variable CLAIM_FLAG is a binary variable. We will use Logistic Regression model to predict the likelihood that a person will crash their car.

### Approach - Logistic Regression: Backward Selection+AIC

We use binary model applied simple backward selection. The initial modeling including all 14 variables and a logit link function in an attempt to identify the regression model that included only significant p-values. The resulting model is to eliminate all non-significant predictors from the model. We will also attempt to identify the model by having the lowest Akaike Information Criterion (AIC) score via backward selection.

```{r}
#Select data: get a subset dataset with ID and 14 independent variables and CLAIM_FLAG only. 
#attribute ID is not a variable, it is just a record indicator.
df_final<-subset(claim_df,select=c(ID,EDUCATION,CAR_AGE,GENDER,RED_CAR,INCOME,
               HOME_VAL,OLDCLAIM,CLM_FREQ,PARENT1,MSTATUS,KIDSDRIV,
               HOMEKIDS,OCCUPATION,CAR_USE,CLAIM_FLAG))

df_new_BKUP=df_final

# Part 3: Build Models
#Split training and testing data: training:60%, testing:40%

library(caTools)

#To set a seed number
set.seed(1)

#Split dataset. 0.6 indicates 60% training and 40% test data.
spl<-sample.split(Y=df_final$ID,SplitRatio = 0.6)
#spl
train=subset(df_final,spl=="TRUE")
test=subset(df_final,spl=="FALSE")

# Note we look at each model AIC number, as the delta AIC between 2 model within 0-2 has a substantial support, 
# delta within 4-7 considerably less support and delta greater than 10 essentially no support.
#
```

#### Step 1: Logistic Regression with all 14 variables.

We see CAR_AGE, RED_CAR and OCCUPATION are not significant predictors. P-values for these 3 variables are greater than 0.05.
```{r}
model<-glm(CLAIM_FLAG~.-ID,data=train,family="binomial")
summary(model)
```

#### Step 2: Eliminate CAR_AGE and Re-apply the Model
```{r}
model<-glm(CLAIM_FLAG~.-ID-CAR_AGE,data=train,family="binomial")
summary(model)
```
Residual deviance: 6369.3  on 6167  degrees of freedom

AIC: 6397.3

Compared with step 1 model, Residual deviance increased. Therefore, CAR_AGE is a significant predictor, it should not be removed.

#### Step 3: Eliminate RED_CAR and Re-apply the Model
```{r}
model<-glm(CLAIM_FLAG~.-ID-RED_CAR,data=train,family="binomial")
summary(model)
```
Residual deviance: 6369.1  on 6167  degrees of freedom

AIC: 6397.1

Compared with step 1 model, Residual deviance increased slightly, and AIC is decreased by about 1 unit. Therefore, RED_CAR cannot be considered for removal.

#### Step 4 : Eliminate OCCUPATION and Re-apply the Model
```{r}
model<-glm(CLAIM_FLAG~.-ID-OCCUPATION,data=train,family="binomial")
summary(model)
```
Residual deviance: 6368.9  on 6167  degrees of freedom

AIC: 6396.9

Compared with step 1, Residual deviance is same but AIC is decreased by 2 units. AIC smaller is better model. Therefore, OCCUPATION variable is not a significant predictor, it can be removed from the dataset.

<!-- 
#### Step 4-1: Remove two non-significant predictors
```{r, echo=FALSE, eval=FALSE}
model<-glm(CLAIM_FLAG~.-ID-OCCUPATION-RED_CAR,data=train,family="binomial")
summary(model)
```
We removed two non-significant predictors RED_CAR and OCCUPATION from dataset and re-apply the model.
Residual deviance: 6369.2  on 6168  degrees of freedom

AIC: 6395.2

Compared with step 1, Residual deviance is increased by 0.3 unit, but AIC is decreased by 3.7units. The model is optimized by removing RED_CAR and OCCUPATION.

-->

#### Step 5: Use the fitted model to do prediction for test data.

```{r}
res=predict(model,data=test,type="response")
#res
```

####Step 6:Create confusion matrix and evaluate the model 

**Note:** Confusion matrix shows the number of correct and incorrect predictions made by the model compared to the actual outcomes (target value) in the data. We used ROC Curve to determine a proper threshold. The threshold that we decided is 0.5.

```{r}
#First:Use ROC Curve to find threshold 
#import library for ROC for finding thr
library(pROC)
#predicted values for training dataset
pred<-predict(model,data=train, type="response")
#defind the RocPred and RocPref
#tpr=ture positive rate.   fpr=falls positive rate
RocPred<-prediction(pred,train$CLAIM_FLAG)
RocPref<-performance(RocPred,"tpr","fpr")
plot(RocPref,colorize=TRUE,print.cutoffs.at=seq(0.1,by=0.1),main="ROC Curve")
```

**Note:** Based on the threshold, we had below confusion matrix. 

```{r}
#Second: creating confusion matrix according to threshold at 0.5
tab<-table(Actualvalue=train$CLAIM_FLAG,Predictedvalue=res>0.5)
tab
confusion_matrix<-sum(diag(tab))/sum(tab)*100
confusion_matrix
```
## Conclusion
We can infer the following from coefficients that we get from the optimized model from step 5
EDUCATION: The higher level education of the policyholder has, the less likely to be involved in an accident. 
CAR_AGE: The older the insured car is, the less likely it is to be involved in a car accident. This result may not be true as we don't know who is driving.

**GENDER:** Gender is also a factor for car accident involving, but here we cannot tell it is male or female more likely to involve into a car accident.

**RED_CAR:** If the insured car is red, more likely to involve to a car accident. This may not be a surprise to us.
INCOME: The higher income that the policyholder has, they are less likely to be involved in an accident. This might to indicate that higher income policyholders drive more responsibly than the lower income drivers. This also could be associate with EDUCATION level.

**HOME-VAL:** Policyholders have more expensive home, drive more responsible. This could associate with INCOME and EDUCATON.

**OLDCLAIM:** Policyholders have past claims payout are likely to be involved in an accident. But we combine observation from plot in part 1 when we did plot for each variable vs CALIM_FLAG:  We saw that policyholders with relatively smaller cumulative past claims payouts are more likely involve to an accident than drivers who have had large claim payouts in the past.

**CLM_FREQ:** The more frequently a driver has had insurance claims in the past, the more likely they are to be involved in an accident. 

**PRENT1:** Policyholders are single parents, more likely to involve to car accident. It may tell us this might be true as their live are more stress than other parents.

**MSTATUS:** Policyholders' marriage status is a factor to car accident. Combine observation from plot in part 1, we found that unmarried policyholders are more likely to be involved in an accident. 

**KIDSDRIV:** If the policyholder has youth to drive the insured vehicle, it is more likely that the car will be involved in an accident. This result doesn't surprise us.

**HOMEKIDS1:** If the policyholder has children living at home with them they are more likely to be involved in an accident. 

**CAR_USE:** The CAR_USE is significant variable to the CLAIM_FLAG. Combine our observation in part 1 plot, drivers of private use are more responsible and less likely to be involved in accidents than are drivers of commercial vehicles. 

### Create Models II: Decision Tree Model
Decision Tree is a classification tree that is used to analysis data when the predicted outcome is the class to which the data belongs.
```{r}
library(MASS)
library(rpart)

#setting the seed number so we get same results each time
#we run decision tree
set.seed(1)

#save df_final as df_dt for Decision Tree
df_dt=df_final

#create the train and test data set: trainDF=60%, testDF=40%
library(caTools)
library(caret)
library(rpart.plot)
#traget variable is CLAIM_FLAG
ind<-sample.split(Y=df_dt$ID,SplitRatio = 0.6)
trainDF=subset(df_dt,ind=="TRUE")
testDF=subset(df_dt,ind=="FALSE")


#fitting the model
dtmodel<-rpart(CLAIM_FLAG~.-ID, data=trainDF,method = "class")
rpart.plot(dtmodel,type = 4,extra = 101)
summary(dtmodel)

#Predictions
PredictDT<-predict(dtmodel,data=testDF,type="class")

tabDT<-table(actual=trainDF$CLAIM_FLAG,predictions=PredictDT)
tabDT

# Confusion matrix
DT_confusionMT<-sum(diag(tabDT))/sum(tabDT)*100
DT_confusionMT
```
###Part 4: Model Selection
When we select a model, we need to compare Accuracy, Sensitivity and Specificity between Logistic Regression and Decision Tree.

Accuracy=(TN+TP)/ total number: Is the percentage of correctly classified in sentence.

Sensitivity=TP/(FN+TP): is the percentage of customer with claim classified correctly.

Specificity = TN/TN+FP: is the percentage of customer without claim classified correctly.

Logistic Regression

Overall Accuracy	(4320+342)/(4320+342+230+1289)*100= 75.42

Sensitivity	342/(1289+342)*100=20.96

Specificity	4320/(4320+230)*100=94.94
	
Decision Tree

Overall Accuracy	(4402+204)/(4402+204+148+1427)*100=74.52

Sensitivity	204/(1427+204)*100=12.5

Specificity	4402/(4402+124)*100=96.74

From above table we see that Logistic Regression has higher Accuracy than Decision Tree. For the Sensitivity, Logistic Regression has about 8% higher than Decision Tree. As we are using model to predict policyholder has claim or not, therefore, Sensitivity is an important key for the model.

Based on the comparison, we select the Logistic Regression Model over the Decision Tree as the best model in this project.

###Additional Approach
Our group decided to use additional two models in this project as a learning object, but we will not compare these two models as we will use different response variable in these two models.
We will not have details to explain these two models in this project.


### Additional Model I: Linear Regression Model


**CLM_AMT** attribute as the response variable, while various subsets of the 4 potential predictor variables were used as independent variables.

Since our response variable CLM_AMT is a numeric variable. We will use Linear Regression model to predict the likelihood that a person will crash their car, hence produce a claim.

#### Step 1: Linear Regression with AGE variable alone.

Here we noted the p-value is: 6.423e-07

```{r, echo=FALSE}
# first we calculate the claim mean value from the dataset.

claim_amt_mean = mean(claim_df$CLM_AMT, na.rm=T)

# we plot the ABline to show where the mean lies

plot(claim_df$CLM_AMT~claim_df$AGE)
abline(h=claim_amt_mean)
```

```{r}
# use lm to fit a regression line through these data:
# Use the AGE value alone to try out the linear model.
# p-value: 5.528e-11

lm_model = lm(CLM_AMT ~ AGE, data=claim_df)
summary(lm_model)
```

#### Step 2: Linear Regression with AGE and the age of the car CAR_AGE variables.

Here we noted the p-value has dramatically decreased with this additional variable, which makes this model more interesting. We noticed a significant drop in p-value from 6.423e-07 to 2.24e-13

```{r}
lm_model = lm(CLM_AMT ~ AGE+CAR_AGE, data=claim_df)
summary(lm_model)
```

####Step 2-1: Plot the Linear Regression Model

```{r}
# Plot a 2x2 graphs:
# 
par(mfrow=c(2,2))
plot(lm_model)
```

#### Step 3: Linear Regression with AGE, the age of the car CAR_AGE and the color red RED_CAR variables.

Here we noted the p-value has increased with this additional variable, which makes color RED not related to any claim scenarios..

```{r}
lm_model = lm(CLM_AMT ~ AGE+CAR_AGE+RED_CAR, data=claim_df)
summary(lm_model)
```

####Step 3-1: Plot the Linear Regression Model

```{r}
# Plot a 2x2 graphs:
# 
par(mfrow=c(2,2))
plot(lm_model)
```

#### Step 4: Linear Regression with AGE, the age of the car CAR_AGE and the occupation of the driver variables.

Here we noted the p-value has again decreased dramatically with this additional variable, which makes occupation var very relavent to any claim scenarios..

```{r}
lm_model = lm(CLM_AMT ~ AGE+CAR_AGE+OCCUPATION, data=claim_df)
summary(lm_model)
```

####Step 4-1: Plot the Linear Regression Model

```{r}
# Plot a 2x2 graphs:
# 
par(mfrow=c(2,2))
plot(lm_model)
```

#### Step 5: Linear Regression with AGE, the age of the car CAR_AGE, the occupation of the driver and driver gender variables.

Here we noted the p-value has again decreased dramatically with this additional variable, which makes gender information very relavent to any claim scenarios..

```{r}
model = lm(CLM_AMT ~ AGE+CAR_AGE+OCCUPATION+GENDER, data=claim_df)
summary(lm_model)
```

####Step 5-1: Plot the Linear Regression Model

```{r}
# Plot a 2x2 graphs:
# 
par(mfrow=c(2,2))
plot(lm_model)
```

### Additional Model II: Random Forest Model
Random forest classifier creates a set of decision trees from randomly selected subset of training set. It combines more than one algorithm. As a practice, our group will have the plot for the Random Forest in this project.

Claim flag RF Plot is shown below.

```{r}
library(randomForest)
# Let's try to predict claim using a random forest.
model_rf<-randomForest(CLAIM_FLAG~.,data=trainDF)
# Let's look at the error
plot(model_rf)
```

```{r, echo=FALSE, eval=FALSE}
pred.train.rf <- predict(model_rf,testDF)
mean(pred.train.rf==testDF$CLAIM_FLAG)
t1<-table(pred.train.rf,testDF$CLAIM_FLAG)
presicion<- t1[1,1]/(sum(t1[1,]))
recall<- t1[1,1]/(sum(t1[,1]))
presicion
recall
F1<- 2*presicion*recall/(presicion+recall)
F1
```



## Appendix

  * [https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/](https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/)

  * [http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software#r-functions](http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software#r-functions)
  
  * [https://www.r-bloggers.com/how-do-i-interpret-the-aic/](https://www.r-bloggers.com/how-do-i-interpret-the-aic/)

  * [https://www.theanalysisfactor.com/r-glm-model-fit/](https://www.theanalysisfactor.com/r-glm-model-fit/)
















